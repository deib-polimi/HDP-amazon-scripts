#!/bin/bash
ansible_cluster_name="cluster";

#copy the template hosts file over the current hosts file
sudo cp /etc/hosts.template /etc/hosts.new
echo "hosts template copied" 

#get the IP of the instance and update the host file
wget -qO- http://instance-data/latest/meta-data/local-ipv4 | xargs -I RES sudo sed 's/local_ip/RES/' -i /etc/hosts.new
echo "master IP updated on hosts file"

sudo cp /etc/hosts.new /etc/hosts
echo "fixed hosts applied" 

#fix the hostname to master
sudo bash /home/ubuntu/scripts/fixhostname.bsh master 
echo "hostname fixed"

#get the access keys  for ec2 console
export AWS_ACCESS_KEY=$(wget -qO- http://instance-data/latest/user-data/ | grep AWS_ACCESS_KEY | cut -d "=" -f2)
export AWS_SECRET_KEY=$(wget -qO- http://instance-data/latest/user-data/ | grep AWS_SECRET_KEY | cut -d "=" -f2)
export JAVA_HOME="/usr/lib/jvm/java-8-oracle"
export EC2_HOME="/usr/local/ec2/ec2-api-tools-1.7.3.0"
export PATH=$PATH:"$EC2_HOME"/bin

echo "access key and secret key retrieved"

#count the number of slaves to assign incremental names
counter=1;
#gets the IPs of the machines running in the same region and assign names as slaves. Updates the hosts file
for ip in $(ec2-describe-instances --region us-west-1 | grep PRIVATEIPADDRESS | cut -f2 | grep -v `wget -qO- http://instance-data/latest/meta-data/local-ipv4`);
do
        echo "$ip slave$counter" | sudo tee --append /etc/hosts
		echo "added slave $counter to the host file"
        counter=$((counter+1))		
done


#remove the known_hosts file
sudo rm /etc/ssh/ssh_known_hosts

#populate the Ansible configuration file
echo "[$ansible_cluster_name]" >  /etc/ansible/hosts
cat /etc/hosts | grep slave | grep -v \#  | cut -d " " -f2  | sudo tee -a /etc/ansible/hosts
echo "updated Ansible configuration"

#add slaves SSH fingerprints to the known hosts to allow Ansible automatic interactions
cat /etc/hosts | grep slave |  grep -v \#  |  xargs ssh-keyscan -H -t ecdsa | sudo tee -a /etc/ssh/ssh_known_hosts
echo "updated fingerprints of slaves"

#create the new keypair and put it in the ubuntu user
ssh-keygen -t rsa -N "" -f /home/ubuntu/id_rsa
sudo mv /home/ubuntu/id_rsa /home/ubuntu/.ssh/id_rsa
sudo mv /home/ubuntu/id_rsa.pub /home/ubuntu/.ssh/id_rsa.pub

#copy over to slaves the public key and update the authorized keys with the script. It also removes the temporary user
echo "copying and installing public key to slaves"
for slave in $(cat /etc/hosts | grep slave |  grep -v \# ) 
do
	sudo sshpass -p "$AWS_ACCESS_KEY" scp /home/ubuntu/.ssh/id_rsa.pub tempuser@$slave:~
	sshpass -p "$AWS_ACCESS_KEY" ssh  tempuser@$slave update_key.bsh
done


echo "distributing host file"
#distribute the hosts file to slaves to fix ips
ansible $ansible_cluster_name -m copy -a "src=/etc/hosts dest=/etc/hosts" --sudo

echo "distributing fingerprints"
#distribute the  known host file so that everyone knows the correct fingerprints
ansible $ansible_cluster_name -m copy -a "src=/etc/ssh/ssh_known_hosts dest=/etc/ssh/ssh_known_hosts" --sudo

echo "assigning slaves their hostname"
#if you don't understand this talk to Srdjan..
cat /etc/hosts | grep slave | grep -v \#  | cut -d " " -f3 | xargs -I J ssh J sudo bash /home/ubuntu/scripts/fixhostname.bsh  J


echo "setting up Ambari to look at the new hostname"
#restart the Ambari agent to reconnect to the master with the new hostname
ansible $ansible_cluster_name -a "ambari-agent restart" --sudo

echo "install ambari server and start it"
sudo apt-get install -y ambari-server
sudo ambari-server setup -j $JAVA_HOME -s
sudo ambari-server start
