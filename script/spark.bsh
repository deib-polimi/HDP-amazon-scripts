##############
# NOT TESTED #
##############


# TODO get name node form ambari
$namenode=""

# set up the configuration (assumption is that the NN is in $namenode)
## set the logging folder
echo 'spark.eventLog.dir               hdfs://$namenode:8020/user/ubuntu/spark-app-logs' | sudo tee --append /etc/spark/conf/spark-defaults.conf

#copy the yarn configuration (assumption is that there is at least one slave)
sudo mkdir /hadoop
sudo scp -r slave1:/etc/hadoop/conf /hadoop 

#set YARN_CONF_DIR env variable
echo 'YARN_CONF_DIR=/hadoop/conf' | sudo tee --append /etc/environment

#change ownership (just in case) (should already be here)
sudo chown -R ubuntu /etc/spark
sudo chmod -R +x /etc/spark/sbin 
sudo chmod -R +x /etc/spark/bin

#push the configuration to all the slaves
#TODO: use cluster name
ansible cumpa -m copy -a "src=/etc/spark dest=/etc" --sudo
ansible cumpa -a "chown -R ubuntu /etc/spark " --sudo
ansible cumpa -a "chmod -R +x /etc/spark/bin" --sudo
ansible cumpa -a "chmod -R +x /etc/spark/sbin" --sudo

#create home folder (assumption, there exists at least 1 slave)
#TODO: what if thhe user did not select hdfs?
ansible slave1 -a "sudo -u hdfs hdfs dfs -mkdir /user/ubuntu"
ansible slave1 -a "sudo -u hdfs hdfs dfs -chown ubuntu /user/ubuntu"

#add the slaves to the spark configuration
cat /etc/hosts | grep slave | grep -v \#  | cut -d " " -f3 > $SPARK_HOME/conf/slaves

#start the master and the slaves
$SPARK_HOME/sbin/start-all.sh

#start the jobserver (assumes that the spark job server is already in the image)
/home/ubuntu/job-server/server_start.sh
